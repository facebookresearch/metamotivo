{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Motivo: online training tutorial\n",
    "This notebook is designed for showcasing how to use the library for training an FB-CPR agent. It is not designed to exactly reproduce the results in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import dataclasses\n",
    "from humenv import make_humenv\n",
    "from humenv.bench.gym_utils.rollouts import rollout\n",
    "import mediapy as media\n",
    "from metamotivo.buffers.buffers import DictBuffer, TrajectoryBuffer\n",
    "from metamotivo.fb_cpr import FBcprAgent, FBcprAgentConfig\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import time\n",
    "from gymnasium import ObservationWrapper\n",
    "\n",
    "from packaging.version import Version\n",
    "\n",
    "if Version(gymnasium.__version__) >= Version(\"1.0\"):\n",
    "    raise RuntimeError(\"This tutorial does not support yet gymnasium >= 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide the time step inside an episode to the agent since it is used to decide when to switch policy (i.e., embedding `z`) in a rollout of the online training. Gymnasium >=1.0 provides this wrapper but here we report a simpler version for compatibility with previous versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAwareObservation(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    The MIT License\n",
    "\n",
    "    Copyright (c) 2016 OpenAI\n",
    "    Copyright (c) 2022 Farama Foundation\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.max_timesteps = env.spec.max_episode_steps\n",
    "        self.timesteps: int = 0\n",
    "        self._time_preprocess_func = lambda time: np.array([time], dtype=np.int32)\n",
    "        time_space = gymnasium.spaces.Box(0, self.max_timesteps, dtype=np.int32)\n",
    "        assert not isinstance(\n",
    "            env.observation_space, (gymnasium.spaces.Dict, gymnasium.spaces.Tuple)\n",
    "        )\n",
    "\n",
    "        observation_space = gymnasium.spaces.Dict(\n",
    "            obs=env.observation_space, time=time_space\n",
    "        )\n",
    "        self._append_data_func = lambda obs, time: {\"obs\": obs, \"time\": time}\n",
    "        self.observation_space = observation_space\n",
    "        self._obs_postprocess_func = lambda obs: obs\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return self._obs_postprocess_func(\n",
    "            self._append_data_func(\n",
    "                observation, self._time_preprocess_func(self.timesteps)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        self.timesteps += 1\n",
    "        return super().step(action)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.timesteps = 0\n",
    "        return super().reset(seed=seed, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and Train parameters\n",
    "\n",
    "We start by defining the parameters of the FB-CPR agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, _ = make_humenv(\n",
    "    num_envs=1,\n",
    "    vectorization_mode=\"sync\",\n",
    "    wrappers=[gymnasium.wrappers.FlattenObservation],\n",
    "    render_width=320,\n",
    "    render_height=320,\n",
    ")\n",
    "\n",
    "agent_config = FBcprAgentConfig()\n",
    "agent_config.model.obs_dim = env.observation_space.shape[0]\n",
    "agent_config.model.action_dim = env.action_space.shape[0]\n",
    "agent_config.model.device = \"cpu\"\n",
    "agent_config.model.norm_obs = True\n",
    "agent_config.model.seq_length = 1\n",
    "# misc\n",
    "agent_config.train.discount = 0.98\n",
    "agent_config.compile = False\n",
    "agent_config.cudagraphs = False\n",
    "agent = FBcprAgent(**dataclasses.asdict(agent_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a few parameters for online training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1_000_000\n",
    "online_parallel_envs = 5\n",
    "log_every_updates = 100\n",
    "online_num_env_steps = 2000\n",
    "num_seed_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Expert\" trajectories\n",
    "FB-CPR leverages expert observation-only trajecteries in the training process. For training Meta Motivo you can use the motion capture dataset as described in the HumEnv repository. Here for simplicity we create \"expert\" trajectories running a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def act(self, *args, **kwargs):\n",
    "        return self.env.action_space.sample()\n",
    "\n",
    "\n",
    "random_agent = RandomAgent(env)\n",
    "_, episodes = rollout(env=env, agent=random_agent, num_episodes=4)\n",
    "for ep in episodes:\n",
    "    ep[\"observation\"] = ep[\"observation\"].astype(np.float32)\n",
    "    del ep[\"action\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize an episode by reloading `qpos` (and optionally `qvel`) information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = episodes[0]\n",
    "frames = []\n",
    "for i in range(len(ep[\"info\"][\"qpos\"])):\n",
    "    env.unwrapped.set_physics(ep[\"info\"][\"qpos\"][i])\n",
    "    frames.append(env.render())\n",
    "media.show_video(frames, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this tutorial we provide a simple buffer for storing trajectories (see `examples/trajecotory_buffer.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_buffer = TrajectoryBuffer(\n",
    "    capacity=len(episodes),\n",
    "    seq_length=agent_config.model.seq_length,\n",
    "    device=agent.device,\n",
    ")\n",
    "expert_buffer.extend(episodes)\n",
    "print(expert_buffer)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "This section describes the training loop that should be self explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env, _ = make_humenv(\n",
    "    num_envs=online_parallel_envs,\n",
    "    vectorization_mode=\"sync\",\n",
    "    wrappers=[\n",
    "        gymnasium.wrappers.FlattenObservation,\n",
    "        lambda env: TimeAwareObservation(env),\n",
    "    ],\n",
    "    render_width=320,\n",
    "    render_height=320,\n",
    ")\n",
    "\n",
    "replay_buffer = {\n",
    "    \"train\": DictBuffer(capacity=buffer_size, device=agent.device),\n",
    "    \"expert_slicer\": expert_buffer,\n",
    "}\n",
    "obs, _ = train_env.reset()\n",
    "print(obs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progb = tqdm(total=online_num_env_steps)\n",
    "td, info = train_env.reset()\n",
    "total_metrics, context = None, None\n",
    "start_time = time.time()\n",
    "for t in range(0, online_num_env_steps, online_parallel_envs):\n",
    "    with torch.no_grad():\n",
    "        obs = torch.tensor(td[\"obs\"], dtype=torch.float32, device=agent.device)\n",
    "        step_count = torch.tensor(td[\"time\"], device=agent.device)\n",
    "        context = agent.maybe_update_rollout_context(z=context, step_count=step_count)\n",
    "        if t < num_seed_steps:\n",
    "            action = train_env.action_space.sample().astype(np.float32)\n",
    "        else:\n",
    "            # this works in inference mode\n",
    "            action = agent.act(obs=obs, z=context, mean=False).cpu().detach().numpy()\n",
    "    new_td, reward, terminated, truncated, new_info = train_env.step(action)\n",
    "    real_next_obs = new_td[\"obs\"].astype(np.float32).copy()\n",
    "    done = np.logical_or(terminated.ravel(), truncated.ravel())\n",
    "    for idx, trunc in enumerate(done):\n",
    "        if trunc:\n",
    "            print(new_info[\"final_observation\"])\n",
    "            real_next_obs[idx] = new_info[\"final_observation\"][idx][\"obs\"].astype(\n",
    "                np.float32\n",
    "            )\n",
    "    data = {\n",
    "        \"observation\": obs,\n",
    "        \"action\": action,\n",
    "        \"z\": context,\n",
    "        \"step_count\": step_count,\n",
    "        \"next\": {\n",
    "            \"observation\": real_next_obs,\n",
    "            \"terminated\": terminated.reshape(-1, 1),\n",
    "            \"truncated\": truncated.reshape(-1, 1),\n",
    "            \"reward\": reward.reshape(-1, 1),\n",
    "        },\n",
    "    }\n",
    "    replay_buffer[\"train\"].extend(data)\n",
    "\n",
    "    metrics = agent.update(replay_buffer, t)\n",
    "\n",
    "    if total_metrics is None:\n",
    "        total_metrics = {k: metrics[k] * 1 for k in metrics.keys()}\n",
    "    else:\n",
    "        total_metrics = {k: total_metrics[k] + metrics[k] for k in metrics.keys()}\n",
    "    if t % log_every_updates == 0:\n",
    "        m_dict = {}\n",
    "        for k in sorted(list(total_metrics.keys())):\n",
    "            tmp = total_metrics[k] / (1 if t == 0 else log_every_updates)\n",
    "            m_dict[k] = np.round(tmp.mean().item(), 6)\n",
    "        m_dict[\"duration\"] = time.time() - start_time\n",
    "        print(f\"Steps: {t}\\n{m_dict}\")\n",
    "        total_metrics = None\n",
    "    progb.update(online_parallel_envs)\n",
    "    td = new_td"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
